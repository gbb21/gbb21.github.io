<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Boost Approach Review</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="经典机器学习算法回顾-之-boost-框架">经典机器学习算法回顾 之 Boost 框架</h1>
<p>在本期文章里，包子君将带大家来快速回顾一下经典 Boost 机器学习算法。</p>
<p>Boost 框架是一种通过累加弱模型来产生一个强模型的方法。它和一般的 Bagging 投票方法相比较，它们的相同点都是累加弱模型，但区别是在投票模型中， 每一个弱模型都是预测最终结果的（通过不同Groups的features），而 Boost 框架中的第 <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?k" alt="k" title="k" /> 个弱模型是预测前面 <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?1...k-1" alt="1...k-1" title="1...k-1" /> 个累加模型与正确答案之间的残差 (residual)， 也就是说 Boost 框架通过不断消除残差来提高模型精度。</p>
<p>Boost 框架的一般形式为：</p>
<p><br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%20f%28x%29%20%3D%20%5Csum_%7Bm%3D1%7D%5EM%20%5Cbeta_m%20g_m%28x%29%20" alt=" f(x) = \sum_{m=1}^M \beta_m g_m(x) " title=" f(x) = \sum_{m=1}^M \beta_m g_m(x) " /><br /></p>
<p><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?M" alt="M" title="M" /> 是总共弱模型个数， <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cbeta_m" alt="\beta_m" title="\beta_m" /> 是每个弱模型的权重， <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?g_m" alt="g_m" title="g_m" />是每个弱模型。 我们也可把它写成递归的形式：</p>
<p><br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%20f_m%28x%29%20%3D%20f_%7Bm-1%7D%28x%29%20%2B%20%5Cbeta_m%20g_m%28x%29%20" alt=" f_m(x) = f_{m-1}(x) + \beta_m g_m(x) " title=" f_m(x) = f_{m-1}(x) + \beta_m g_m(x) " /><br /></p>
<p>Boost 框架训练的目标为：</p>
<p><br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%20argmin_%7B%5Cbeta_m%2Cg_m%7D%20%5Csum_%7Bn%3D1%7D%5E%7BN%7D%20L%28y_n%2C%20f_%7Bm-1%7D%28x_n%29%20%2B%20%5Cbeta_m%20g_m%28x_n%29%29%20" alt=" argmin_{\beta_m,g_m} \sum_{n=1}^{N} L(y_n, f_{m-1}(x_n) + \beta_m g_m(x_n)) " title=" argmin_{\beta_m,g_m} \sum_{n=1}^{N} L(y_n, f_{m-1}(x_n) + \beta_m g_m(x_n)) " /><br /></p>
<p><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?N" alt="N" title="N" /> 是总共训练样例的数目， <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?L" alt="L" title="L" /> 是 Loss 函数。在优化时，我们采取迭代增加弱模型的方法, 用第 <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?m" alt="m" title="m" /> 个模型去拟合每次前面 <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?m-1" alt="m-1" title="m-1" /> 模型和的残差。</p>
<p>对于 Boost 框架，我们有两种常见的应用，分别是 Ada-Boost 自适应增强分类器 和 GDBT 增强回归树。</p>
<h2 id="ada-boost-自适应增强分类器">Ada-Boost 自适应增强分类器</h2>
<p>对于输入 <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?x_n%20%5Cin%20%5Cmathbf%7BR%7D%5Ed%2C%20y_n%20%5Cin%20%5C%7B-1%2C%20%2B1%5C%7D" alt="x_n \in \mathbf{R}^d, y_n \in \{-1, +1\}" title="x_n \in \mathbf{R}^d, y_n \in \{-1, +1\}" /> 以及弱分类器集合 <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BG%7D%20%3D%20%5C%7Bg_1%2C%20g_2%20%5Cdots%20g_k%5C%7D" alt="\mathbf{G} = \{g_1, g_2 \dots g_k\}" title="\mathbf{G} = \{g_1, g_2 \dots g_k\}" /> ， Ada-Boost 会通过Boost 框架从 <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?K" alt="K" title="K" /> 个弱分类器中找到 <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?M" alt="M" title="M" /> 个最佳弱分类器并分配其权重来优化一个指数型损失函数。</p>
<p>为了选择第 <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?m" alt="m" title="m" /> 个弱分类器及其权重，我们先假设已经得到了前面 <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?m%20-%201" alt="m - 1" title="m - 1" /> 个，于是损失函数成为：</p>
<p><br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%20%5Csum_%7Bn%3D1%7D%5E%7BN%7D%20e%5E%7B-y_n%20%28%5Cbeta_1%20g_1%28x_n%29%20%2B%20%5Cbeta_2%20g_2%28x_n%29%20%2B%20%5Cdots%20%2B%20%5Cbeta_m%20g_m%28x_n%29%29%7D%0A%3D%20%5Csum_%7Bn%3D1%7D%5E%7BN%7D%20F_%7Bm-1%7D%28x_n%29%20e%5E%7B-y_n%20%5Cbeta_m%20f_m%28x_n%29%7D%20" alt=" \sum_{n=1}^{N} e^{-y_n (\beta_1 g_1(x_n) + \beta_2 g_2(x_n) + \dots + \beta_m g_m(x_n))}
= \sum_{n=1}^{N} F_{m-1}(x_n) e^{-y_n \beta_m f_m(x_n)} " title=" \sum_{n=1}^{N} e^{-y_n (\beta_1 g_1(x_n) + \beta_2 g_2(x_n) + \dots + \beta_m g_m(x_n))}
= \sum_{n=1}^{N} F_{m-1}(x_n) e^{-y_n \beta_m f_m(x_n)} " /><br /></p>
<p>我们通过尝试 <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?M" alt="M" title="M" /> 个不同的弱分类器（假设 <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cbeta_m%20%3E%200" alt="\beta_m &gt; 0" title="\beta_m &gt; 0" /> ），可以找到，损失函数最小的那个，并且通过对 <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cbeta_m" alt="\beta_m" title="\beta_m" /> 求导等于零来得到其系数的值：</p>
<p><br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%20%5Cbeta_m%20%3D%20%5Cfrac%7B1%7D%7B2%7Dlog%28%5Cfrac%7B1%20-%20%5Cepsilon_m%7D%7B%5Cepsilon_m%7D%29%20" alt=" \beta_m = \frac{1}{2}log(\frac{1 - \epsilon_m}{\epsilon_m}) " title=" \beta_m = \frac{1}{2}log(\frac{1 - \epsilon_m}{\epsilon_m}) " /><br /></p>
<p>其中：</p>
<p><br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%20%5Cepsilon_m%20%3D%20%5Cfrac%20%7B%5Csum_%7By_n%20%5Cneq%20g_m%7D%20F_%7Bm-1%7D%28x_n%29%7D%7B%5Csum_%7Bn%3D1%7D%5E%7BN%7DF_%7Bm-1%7D%28x_n%29%7D%20" alt=" \epsilon_m = \frac {\sum_{y_n \neq g_m} F_{m-1}(x_n)}{\sum_{n=1}^{N}F_{m-1}(x_n)} " title=" \epsilon_m = \frac {\sum_{y_n \neq g_m} F_{m-1}(x_n)}{\sum_{n=1}^{N}F_{m-1}(x_n)} " /><br /></p>
<p>迭代以上步骤 <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?M" alt="M" title="M" /> 次即可得到由 <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?M" alt="M" title="M" /> 个弱分类器组成的强分类器。</p>
<h2 id="gdbt-增强回归树">GDBT 增强回归树</h2>
<p>增强回归树是通过之前提到的 Boost 方法，不断累加弱回归树来得到一个强的回归模型。 唯一的区别是，我们用导函数去近似残差，把 <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?m" alt="m" title="m" /> 棵回归树拟合成前面 <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?m-1" alt="m-1" title="m-1" /> 棵树和的负导数，于是我们有：</p>
<p><br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%20%5Cfrac%7B%5Cpartial%20L%28F_%7Bm-1%7D%28x_n%29%29%7D%7B%5Cpartial%20F_%7Bm%20-%201%7D%28x_n%29%7D%20%3D%20-%20%5Cbeta_m%20g_m%28x_n%29%20" alt=" \frac{\partial L(F_{m-1}(x_n))}{\partial F_{m - 1}(x_n)} = - \beta_m g_m(x_n) " title=" \frac{\partial L(F_{m-1}(x_n))}{\partial F_{m - 1}(x_n)} = - \beta_m g_m(x_n) " /><br /></p>
<p>对于回归问题，我们可以用 Least Square 来作为作为损失函数：</p>
<p><br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?L%20%3D%20%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20%28F_m%28x_i%29%20-%20y_i%29%5E2" alt="L = \sum_{i=1}^{n} (F_m(x_i) - y_i)^2" title="L = \sum_{i=1}^{n} (F_m(x_i) - y_i)^2" /><br /></p>
<p>同样我们用最小二乘回归树来拟合 <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?g_m" alt="g_m" title="g_m" /> 函数，在这里我们不详细展开讨论回归树是如何构建的。简单地说，最小二乘回归树通过不断地 branch， 把输入空间中的 <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?N" alt="N" title="N" /> 个点 <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5C%7Bx_n%20%5Cmid%20n%20%5Cin%201%20%5Cdots%20N%20%5C%7D" alt="\{x_n \mid n \in 1 \dots N \}" title="\{x_n \mid n \in 1 \dots N \}" /> 映射到 <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?T" alt="T" title="T" /> 个空间 <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5C%7BR_t%20%5Cmid%20n%20%5Cin%201%20%5Cdots%20T%5C%7D" alt="\{R_t \mid n \in 1 \dots T\}" title="\{R_t \mid n \in 1 \dots T\}" /> 里， 并给每个空间 assign 一个值 <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?R_t" alt="R_t" title="R_t" />，使得下面的回归树损失函数最小化：</p>
<p><br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%20%5Csum_n%20L%28g_m%28x_n%29%2C%20%5Csum_t%20R_t%20I_t%28x_n%29%29%20" alt=" \sum_n L(g_m(x_n), \sum_t R_t I_t(x_n)) " title=" \sum_n L(g_m(x_n), \sum_t R_t I_t(x_n)) " /><br /></p>
<p>其中 <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?I_t%28x_n%29" alt="I_t(x_n)" title="I_t(x_n)" /> 是一个指标函数，当 <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?x_n" alt="x_n" title="x_n" /> 被划分为第 <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?t" alt="t" title="t" /> 个 Region 的时候 <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?I_t%28x_n%29" alt="I_t(x_n)" title="I_t(x_n)" /> 为 1，其余情况全为 0 。</p>
<p>每一轮迭代我们就用产生一棵新的回归树 <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?g_m" alt="g_m" title="g_m" /> 与之前所训练的 <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?m%20-%201" alt="m - 1" title="m - 1" /> 棵树相加，一直到迭代结束。</p>
<h2 id="boost-框架的问题及改进">Boost 框架的问题及改进</h2>
<p>在使用中，我们发现 Boost 方法能够很灵活地拟合各种复杂的训练样本，但在泛化方面却有一定的问题。 Boost 框架和 以随机森林 (Random Forest) 为代表的 Bagging 方法同为 模型 Ensemble 的思路，却着重优化了两个不同的方面：偏差 (Bias) 和 方差 (Variance)。</p>
<p>对于 Boost 方法来说， 由于每一个弱分类器都为了是减少上一个弱分类器在训练样本里的偏差，所以最终 Ensemble 的 偏差 会较小，也就是模型比较灵活 (flexible)； 而对于 Bagging 的方法来说恰恰相反，每一弱分类器都独立预测了最终的结果，通过平均结果的方法，我们把预测结果的方差也减少到了原来每个弱分类器的 <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B%5Csqrt%7BM%7D%7D" alt="\frac{1}{\sqrt{M}}" title="\frac{1}{\sqrt{M}}" /> 。 为了提高 Boost 方法的泛化性能, 我们常常会在训练时，对其加入多个 Regularization 约束，并通过 Early Stopping 的方式来防止 Over-Fitting 。</p>
<p>好了，这次的 Boost 框架包子君就先带大家回顾到这里，如有还有什么不明白的，欢迎在留言中提问哦。</p>
</body>
</html>
